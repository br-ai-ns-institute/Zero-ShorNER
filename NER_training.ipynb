{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a26bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class aims to create the appropriate type of tensor data required as input to the Trainer method.\n",
    "# Takes a BERT-tokenized and \"aligned\" object and returns the dataset class.\n",
    "\n",
    "class Preoblikuj_u_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ee8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_pickle(pickle_name):\n",
    "    with open(pickle_name, 'rb') as fh:\n",
    "        unpickled_object = pickle.load(fh)\n",
    "    return unpickled_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function takes the dataset and the name of the file in which it should be pickled.\n",
    "# The third argument (when explicitly called) forms a subfolder of the BioNER class name that is \"hidden\"\n",
    "# for zero-shot and few-shot training, which enables a transparent folder structure of the data.\n",
    "\n",
    "# Returns nothing, just saves the pickle dataset.\n",
    "\n",
    "def dump_to_pickle(data_set, file_name, class_name=None):\n",
    "    if class_name == None:\n",
    "        folders = os.path.join('Datasets')\n",
    "    else:\n",
    "        folders = os.path.join('Datasets', class_name)\n",
    "    os.makedirs(folders, exist_ok=True)\n",
    "    filename = file_name+'.pkl'\n",
    "    file_path = os.path.join(folders, filename)\n",
    "    outfile = open(file_path,'wb')\n",
    "    pickle.dump(data_set,outfile, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function takes only the name of the class that is \"hidden\" for the purposes of forming datasets\n",
    "# for zero- and few-shot training on that class.\n",
    "\n",
    "\"\"\"\n",
    "Dumps 5 pickled datasets into the appropriate files for the given \"hidden\" class:\n",
    "    \n",
    "     1. dataset_withoutOne_train - dataset for zero-shot training, from which all representatives of the \"hidden\" class are excluded\n",
    "     2. dataset_withOne_test - dataset for testing*, in which there are only representatives of the \"hidden\" class\n",
    "        \n",
    "     3. dataset_UnseenClass_train_1 - 1-shot training dataset, in which there is 1 non-empty representative of the \"hidden\" class\n",
    "     4. dataset_UnseenClass_train_10 - 10-shot training dataset, which contains 10 non-empty representatives of the \"hidden\" class\n",
    "     5. dataset_UnseenClass_train_100 - 100-shot training dataset, which contains 100 non-empty representatives of the \"hidden\" class\n",
    "    \n",
    "     NOTE: For each zero- and few-shot training, each class is the SAME validation dataset.\n",
    "     * The testing dataset per \"hidden\" class is the same for zero-shot and few-shot, so there is only one!\n",
    "     ** training representatives - refers to examples from the initial training set - the one that makes up 85% of the total set\n",
    "\"\"\"\n",
    "\n",
    "def skriva_Dataset_klasu_vraca_TrainValidTest_Subsets(class_name):\n",
    "    indexes_train = list(df_train.index[df_train['klasa']!=class_name])\n",
    "    indexes_test = list(df_test.index[df_test['klasa']==class_name])\n",
    "    \n",
    "    dataset_withoutOne_train = Subset(dataset_train, indexes_train)\n",
    "    dataset_withOne_test = Subset(dataset_test, indexes_test)\n",
    "    \n",
    "    dump_to_pickle(dataset_withoutOne_train, 'dataset_withoutOne_train', class_name=class_name)\n",
    "    dump_to_pickle(dataset_withOne_test, 'dataset_withOne_test', class_name=class_name)\n",
    "    \n",
    "    lst_indexes_FewShot_train = list(df_train.index[(df_train['klasa']==class_name) & (df_train['labels'].apply(lambda lst : sum(lst))>0)])\n",
    "    \n",
    "    indexes_UnseenClass_train_1 = random.choices(lst_indexes_FewShot_train, k=1)\n",
    "    indexes_UnseenClass_train_10 = random.choices(lst_indexes_FewShot_train, k=10)\n",
    "    indexes_UnseenClass_train_100 = random.choices(lst_indexes_FewShot_train, k=100)\n",
    "\n",
    "    dataset_UnseenClass_train_1 = Subset(dataset_train, indexes_UnseenClass_train_1)\n",
    "    dataset_UnseenClass_train_10 = Subset(dataset_train, indexes_UnseenClass_train_10)\n",
    "    dataset_UnseenClass_train_100 = Subset(dataset_train, indexes_UnseenClass_train_100)\n",
    "    \n",
    "    dump_to_pickle(dataset_UnseenClass_train_1, 'dataset_UnseenClass_train_1', class_name=class_name)\n",
    "    dump_to_pickle(dataset_UnseenClass_train_10, 'dataset_UnseenClass_train_10', class_name=class_name)\n",
    "    dump_to_pickle(dataset_UnseenClass_train_100, 'dataset_UnseenClass_train_100', class_name=class_name)\n",
    "    \n",
    "    return dataset_withoutOne_train, dataset_withOne_test, dataset_UnseenClass_train_1, dataset_UnseenClass_train_10, dataset_UnseenClass_train_100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_name = './ALL_DATA_klasa_nova_train.pkl'\n",
    "df_test_name = './ALL_DATA_klasa_nova_test.pkl'\n",
    "\n",
    "df_train = load_from_pickle(df_train_name)\n",
    "df_test = load_from_pickle(df_test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = load_from_pickle('./Datasets/dataset_train.pkl')\n",
    "dataset_valid = load_from_pickle('./Datasets/dataset_valid.pkl')\n",
    "dataset_test = load_from_pickle('./Datasets/dataset_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unseen = 'Drug' # change class to desired for \"hiding\" for zero- and few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a98aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train0shot, _, train1shot, train10shot, train100shot = skriva_Dataset_klasu_vraca_TrainValidTest_Subsets(class_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='dmis-lab/biobert-v1.1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # load the specific tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./Results'+class_unseen+'ZeroShot',   # output folder (folder to store the results)\n",
    "    num_train_epochs=6,                               # number of training epochs\n",
    "    per_device_train_batch_size=16,                   # batch size per device during training\n",
    "    per_device_eval_batch_size=16,                    # batch size for evaluation\n",
    "    weight_decay=0.01,                                # strength of weight decay\n",
    "    logging_dir='./Logs'+class_unseen+'ZeroShot',     # folder to store the logs\n",
    "    #logging_steps=10000,\n",
    "    #logging_strategy='steps',\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    load_best_model_at_end=True \n",
    ")\n",
    "\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                 # pre-trained model for fine-tuning\n",
    "    args=training_args,          # training arguments defined above\n",
    "    train_dataset=train0shot,    # dataset class object for training\n",
    "    eval_dataset=valid_dataset   # dataset class object for validation\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "total_time = time.time()-start_time\n",
    "\n",
    "model_path = os.path.join('Results', class_unseen, 'ZeroShot', 'Model')\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model.save_pretrained(model_path)\n",
    "                \n",
    "tokenizer_path = os.path.join('Results', class_unseen, 'ZeroShot','Tokenizer')\n",
    "os.makedirs(tokenizer_path, exist_ok=True)\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [train1shot, train10shot, train100shot]:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./Results'+class_unseen+'FewShot'+str(i),  # output folder (folder to store the results)\n",
    "        num_train_epochs=10,                                   # number of training epochs\n",
    "        per_device_train_batch_size=16,                        # batch size per device during training\n",
    "        per_device_eval_batch_size=16,                         # batch size for evaluation\n",
    "        weight_decay=0.01,                                     # strength of weight decay\n",
    "        logging_dir='./Logs'+class_unseen+'FewShot'+str(i),    # folder to store the logs\n",
    "        #logging_steps=10000,\n",
    "        #logging_strategy='steps',\n",
    "        save_strategy='epoch',\n",
    "        evaluation_strategy='epoch',\n",
    "        load_best_model_at_end=True \n",
    "    )\n",
    "\n",
    "    model0 = BertForTokenClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model0,                # pre-trained model for fine-tuning\n",
    "        args=training_args,          # training arguments defined above\n",
    "        train_dataset=train_0shot,   # dataset class object for training\n",
    "        eval_dataset=valid_dataset   # dataset class object for validation\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    total_time = time.time()-start_time\n",
    "\n",
    "    model_path = os.path.join('Results', class_unseen, 'FewShot',str(i), 'Model')\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model.save_pretrained(model_path)\n",
    "\n",
    "    tokenizer_path = os.path.join('Results', class_unseen, 'FewShot', str(i), 'Tokenizer')\n",
    "    os.makedirs(tokenizer_path, exist_ok=True)\n",
    "    tokenizer.save_pretrained(tokenizer_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
