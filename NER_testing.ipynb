{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf104fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, BertForTokenClassification  \n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score,recall_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a92e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_pickle(pickle_name):\n",
    "    with open(pickle_name, 'rb') as fh:\n",
    "        unpickled_object = pickle.load(fh)\n",
    "    return unpickled_object   \n",
    "\n",
    "\n",
    "def dump_to_pickle(data_set, file_name, class_name=None):\n",
    "    if class_name == None:\n",
    "        folders = os.path.join('Datasets')\n",
    "    else:\n",
    "        folders = os.path.join('Datasets', class_name)\n",
    "    os.makedirs(folders, exist_ok=True)\n",
    "    filename = file_name+'.pkl'\n",
    "    file_path = os.path.join(folders, filename)\n",
    "    outfile = open(file_path,'wb')\n",
    "    pickle.dump(data_set,outfile, protocol=4)\n",
    "\n",
    "    \n",
    "class Preoblikuj_u_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a61dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing1(test_set_path, model_path, df_test_path):\n",
    "        \n",
    "    testset = load_from_pickle(test_set_path)\n",
    "    df_test = pd.read_pickle(df_test_path)\n",
    "\n",
    "    model = BertForTokenClassification.from_pretrained(model_path, num_labels=2)\n",
    "    args = TrainingArguments(output_dir='./evaldir', per_device_eval_batch_size=16)\n",
    "    \n",
    "    evaler = Trainer(\n",
    "        args=args,\n",
    "        model=model\n",
    "    )\n",
    "    \n",
    "    pred=evaler.predict(testset)\n",
    "    test_indexs_new=df_test.index.to_list()                   \n",
    "    wids=np.array([testset.dataset.encodings.encodings[ii].word_ids for ii in test_indexs_new])\n",
    "    wids[wids==None]=-1\n",
    "    wids=wids.astype(int)\n",
    "  \n",
    "    type_ids=np.array([testset.dataset.encodings.encodings[ii].type_ids for ii in test_indexs_new],dtype=bool)\n",
    "    pre=pred[0].argmax(axis=-1)\n",
    "    pre_list=[]\n",
    "    test_list=[]\n",
    "    for ii in range(wids.shape[0]):\n",
    "        test_list.append(wids[ii][type_ids[ii]])\n",
    "        pre_list.append(pre[ii][type_ids[ii]])\n",
    "        \n",
    "    labels=[]\n",
    "    for ii in range(len(pre_list)):\n",
    "        labels.append(np.array(range(test_list[ii].max()+1)))\n",
    "        for jj in labels[ii]:\n",
    "            bb=np.where(test_list[ii]==jj)[0]\n",
    "            labela=np.array(pre_list[ii])[bb].mean()\n",
    "            if labela>0.01:\n",
    "                labels[ii][jj]=1\n",
    "            else:\n",
    "                labels[ii][jj]=0\n",
    "  \n",
    "    labels_original=list(df_test.iloc[:]['labels']) \n",
    "    f1av=0.0\n",
    "    lf1=0\n",
    "    \n",
    "    for ii in range(len(labels)):\n",
    "        if len(labels[ii])==len(labels_original[ii]):\n",
    "            f1av=f1av+f1_score( labels_original[ii], labels[ii],average=None)\n",
    "            lf1=lf1+1\n",
    "        else:\n",
    "            print(ii, len(labels[ii]), len(labels_original[ii]))\n",
    "    \n",
    "    lab=np.array([])       # predicted labels\n",
    "    labor=np.array([])     # true labels labele\n",
    "    for ii in range(len(labels)):\n",
    "        if len(labels[ii])==len(labels_original[ii]):\n",
    "            lab=np.concatenate((lab,labels[ii]))\n",
    "            labor=np.concatenate((labor,labels_original[ii]))\n",
    "            \n",
    "    accuracy = accuracy_score(labor, lab, normalize=True)\n",
    "    precision = precision_score(labor, lab, average=None)\n",
    "    recall = recall_score(labor, lab, average=None)\n",
    "    f1 = f1_score(labor, lab, average=None)\n",
    "    matrix = confusion_matrix(labor, lab)\n",
    "    \n",
    "    return [accuracy,precision,recall,f1,matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1822c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './Results/Drug/0Shot/Model'\n",
    "test_set_path = './Datasets/Drug/dataset_withOne_test_NULA.pkl'\n",
    "df_test_path = './Datasets/Drug/df_test_UnseenClass.pkl'\n",
    "res = testing1(test_set_path, model_path, df_test_path) \n",
    "\n",
    "with open('./Results/Drug/0Shot/metrike_linguisticNEW.pkl','wb') as outfile:\n",
    "    pickle.dump(res, outfile)\n",
    "\n",
    "metrike=load_from_pickle('./Results/Drug/0Shot/metrike_linguisticNEW.pkl')\n",
    "metrike"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
