{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44fa636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2b860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='dmis-lab/biobert-v1.1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # uƒçitavanje konketnog tokenizatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e78884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_pickle(pickle_name):\n",
    "    with open(pickle_name, 'rb') as fh:\n",
    "        unpickled_object = pickle.load(fh)\n",
    "    return unpickled_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a28b50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function takes the dataset and the name of the file in which it should be pickled.\n",
    "# The third argument (when explicitly called) forms a subfolder of the BioNER class name that is \"hidden\"\n",
    "# for zero-shot and few-shot training, which enables a transparent folder structure of the data.\n",
    "\n",
    "# Returns nothing, just saves the pickle dataset.\n",
    "\n",
    "def dump_to_pickle(data_set, file_name, class_name=None):\n",
    "    if class_name == None:\n",
    "        folders = os.path.join('Datasets')\n",
    "    else:\n",
    "        folders = os.path.join('Datasets', class_name)\n",
    "    os.makedirs(folders, exist_ok=True)\n",
    "    filename = file_name+'.pkl'\n",
    "    file_path = os.path.join(folders, filename)\n",
    "    outfile = open(file_path,'wb')\n",
    "    pickle.dump(data_set,outfile, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "196d2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function BERT-tokenizes (WordPiece principle) input\n",
    "# in our case 2 sequences ((1) explicitly given class and (2) sentence)\n",
    "# and returns the BERT-tokenized output\n",
    "\n",
    "def BERTtokenizovanje_ClassText(df):\n",
    "    \n",
    "    tokenized_encodings = tokenizer(df[\"class\"].to_list(),\n",
    "                                 df[\"text\"].to_list(),\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True,\n",
    "                                 add_special_tokens=True,\n",
    "                                 padding='max_length',\n",
    "                                 max_length=512)\n",
    "    return tokenized_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a1ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the nature of BERT-tokenization (the WordPiece approach may return the initial token as multiple tokens),\n",
    "# alignment of labels is required. In addition, it was necessary to take into account that the sequence was created\n",
    "# by concatenating 2 initial sequences (classes and sentences)\n",
    "\n",
    "# In our approach, we opted for a principle where each part of a unique initial token is assigned a value\n",
    "# (another approach would be to assign a value to only the first part, and to give the rest a value of -100 (corresponds to None)).\n",
    "\n",
    "# The function takes 2 parameters (the third one is assigned due to access selection) - the loaded dataframe and the BERT-tokenized inut\n",
    "# and returns output that now contains aligned labels.\n",
    "\n",
    "def poravnanje_labela(df, tokenized_encodings, label_all_tokens = True):\n",
    "    \n",
    "    labels = list()\n",
    "    for i, label in enumerate(df['labels']):\n",
    "        word_ids = tokenized_encodings.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                if word_idx < len(label):\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        stop = label_ids.index(-100, 2) # drugo javljanje None (-100) je na ovom indexu\n",
    "        label_ids = label_ids[:1] + [1 for x in label_ids[1:stop]] + label_ids[stop:]\n",
    "        labels.append(label_ids)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e8fe5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class aims to create the appropriate type of tensor data required as input to the Trainer method.\n",
    "# Takes a BERT-tokenized and \"aligned\" object and returns the dataset class.\n",
    "\n",
    "class Preoblikuj_u_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a1cb81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d45f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call when the script and 3 (train, valid, test) dataframes are in the same folder (possibly change the .pkl names)\n",
    "\n",
    "df_train_name = './ALL_DATA_klasa_nova_train.pkl'\n",
    "df_valid_name = './ALL_DATA_klasa_nova_valid.pkl'\n",
    "df_test_name = './ALL_DATA_klasa_nova_test.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e592c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_from_pickle(df_train_name)\n",
    "df_valid = load_from_pickle(df_valid_name)\n",
    "df_test = load_from_pickle(df_test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2007313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_encodings_train = BERTtokenizovanje_ClassText(df_train)\n",
    "tokenized_encodings_valid = BERTtokenizovanje_ClassText(df_valid)\n",
    "tokenized_encodings_test = BERTtokenizovanje_ClassText(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95aee2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = poravnanje_labela(df_train, tokenized_encodings_train, label_all_tokens = True)\n",
    "labels_valid = poravnanje_labela(df_valid, tokenized_encodings_valid, label_all_tokens = True)\n",
    "labels_test = poravnanje_labela(df_test, tokenized_encodings_test, label_all_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51945a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Preoblikuj_u_Dataset(tokenized_encodings_train, labels_train)\n",
    "dataset_valid = Preoblikuj_u_Dataset(tokenized_encodings_valid, labels_valid)\n",
    "dataset_test = Preoblikuj_u_Dataset(tokenized_encodings_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c9566",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_to_pickle(dataset_train, 'dataset_train')\n",
    "dump_to_pickle(dataset_valid, 'dataset_valid')\n",
    "dump_to_pickle(dataset_test, 'dataset_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
